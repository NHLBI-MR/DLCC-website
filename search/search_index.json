{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Deep Learning Crash Course To spread the knowledge of deep learning among domain experts. Overview Deep learning is emerging as a major disruptive technology in biomedical and clinical research. It is also a skill with high demand in the decade to come. This course aims to teach the foundations to understand how neural network works and also introduce latest developments. You will build your own neural networks and gain skills to apply deep learning to your field. Why was this course developed There are a few very good deep learning courses (e.g. Stanford CS230 , Stanford CS231n , CMU Into to DL etc.). But for the general audiences, the related costs are high and seats are limited. There are strong interests in the community where I am working (such as, NHLBI, NIH ) for deep learning. Many trainees and colleagues want to apply this technique, from biology to microscopy, from clinical imaging to epidemiology. As the deep learning has not yet penetrated these fields, it is of great interest to expect breakthroughs by applying deep learning there. This course is designed for domain experts to provide in-depth review of deep learning technique. The expected outcome is that one can start apply DL to his/her unique problems after completion of course materials and assignments. As advocated in this great post , add deep learning to your expertise! Who is this course for This course is for anyone who is willing to learn the deep learning. Before diving into the subjects, it is the best to review basic linear algebra and probability theory. Python programming is used through out the course, so getting familiar with python is necessary. Some good materials for mathematics and python programming: Stanford CS229-linalg Dive Into DL, math appendix Learning Python Python Crash Course, one of the easiest tutorial Numpy tutorial Debug Python using vscode These materials contain sufficient information to understand and apply deep learning. After reviewing them, you will be prepared to start this journal. Course content Neural Network basics, backprop, optimization, loss function, setup and monitor your training, convolution neural network, recurrent neural network, transformer, attention mechanism, generative adversarial network, adversarial attacking, visualization of neural network, transfer learning, meta learning, contrastive learning, data management, experiment management, etc. Who will teach Hui Xue is an active researcher on developing deep learning based imaging applications. The AI imaging products he developed had been deployed at more than forty hospitals globally and used daily to help cardiologists. David Hansen is the founder and technical director of Gradient Software, Inc. He is an experienced developer in deep learning. Course offerings \ud83d\udcef NHLBI 2021 \ud83e\uddec BIO398.81 Practical Deep Learning, FAES 2021","title":"Home"},{"location":"#deep-learning-crash-course","text":"To spread the knowledge of deep learning among domain experts.","title":"Deep Learning Crash Course"},{"location":"#overview","text":"Deep learning is emerging as a major disruptive technology in biomedical and clinical research. It is also a skill with high demand in the decade to come. This course aims to teach the foundations to understand how neural network works and also introduce latest developments. You will build your own neural networks and gain skills to apply deep learning to your field.","title":"Overview"},{"location":"#why-was-this-course-developed","text":"There are a few very good deep learning courses (e.g. Stanford CS230 , Stanford CS231n , CMU Into to DL etc.). But for the general audiences, the related costs are high and seats are limited. There are strong interests in the community where I am working (such as, NHLBI, NIH ) for deep learning. Many trainees and colleagues want to apply this technique, from biology to microscopy, from clinical imaging to epidemiology. As the deep learning has not yet penetrated these fields, it is of great interest to expect breakthroughs by applying deep learning there. This course is designed for domain experts to provide in-depth review of deep learning technique. The expected outcome is that one can start apply DL to his/her unique problems after completion of course materials and assignments. As advocated in this great post , add deep learning to your expertise!","title":"Why was this course developed"},{"location":"#who-is-this-course-for","text":"This course is for anyone who is willing to learn the deep learning. Before diving into the subjects, it is the best to review basic linear algebra and probability theory. Python programming is used through out the course, so getting familiar with python is necessary. Some good materials for mathematics and python programming: Stanford CS229-linalg Dive Into DL, math appendix Learning Python Python Crash Course, one of the easiest tutorial Numpy tutorial Debug Python using vscode These materials contain sufficient information to understand and apply deep learning. After reviewing them, you will be prepared to start this journal.","title":"Who is this course for"},{"location":"#course-content","text":"Neural Network basics, backprop, optimization, loss function, setup and monitor your training, convolution neural network, recurrent neural network, transformer, attention mechanism, generative adversarial network, adversarial attacking, visualization of neural network, transfer learning, meta learning, contrastive learning, data management, experiment management, etc.","title":"Course content"},{"location":"#who-will-teach","text":"Hui Xue is an active researcher on developing deep learning based imaging applications. The AI imaging products he developed had been deployed at more than forty hospitals globally and used daily to help cardiologists. David Hansen is the founder and technical director of Gradient Software, Inc. He is an experienced developer in deep learning.","title":"Who will teach"},{"location":"#course-offerings","text":"\ud83d\udcef NHLBI 2021 \ud83e\uddec BIO398.81 Practical Deep Learning, FAES 2021","title":"Course offerings"},{"location":"about/","text":"Thank you for visiting DLCC Please forward comments to : instructors@deeplearningcrashcourse.org Site manager Hui Xue hui.xue@nih.gov NHLBI, NIH Bethesda, Maryland USA 20814","title":"About"},{"location":"about/#thank-you-for-visiting-dlcc","text":"Please forward comments to : instructors@deeplearningcrashcourse.org","title":"Thank you for visiting DLCC"},{"location":"about/#site-manager","text":"Hui Xue hui.xue@nih.gov NHLBI, NIH Bethesda, Maryland USA 20814","title":"Site manager"},{"location":"faes2021/","text":"FAES, 2021 Fall This course is offered at the Foundation For Advanced Education and Science ( FAES@NIH ) in the fall term, 2021 to trainees and colleagues in NIH and collaborative colleges. This offering is under the course name Practical Deep Learning . Practical Deep Learning","title":"FAES 2021"},{"location":"faes2021/#faes-2021-fall","text":"This course is offered at the Foundation For Advanced Education and Science ( FAES@NIH ) in the fall term, 2021 to trainees and colleagues in NIH and collaborative colleges. This offering is under the course name Practical Deep Learning . Practical Deep Learning","title":"FAES, 2021 Fall"},{"location":"nhlbi2021/","text":"NHLBI, 2021 Fall This offering is for Division of Intramural (DIR), NHLBI in Fall, 2021. The course consists of 12 lectures. We will start from basics of neural networks, introduce the loss function, optimization and how to setup and manage the training session. The next section is the convolutional neural network for imaging and vision tasks. We will learn the recurrent neural network (RNN) for the sequence data. More recently, attention mechanism and transformer models (BERT, GPT family etc.) are very popular. They are introduced after RNN. We will teach generative model and in details the GAN (generative adversarial network). The technique to visualize the neural network is introduced to help understand how and why the neural network works. The course will end with a focus on how to handle \"small dataset\" usecase, as in many practical applications, we may not be able to acquire large labelled dataset. Three techniques are introduced, transfer learning, meta learning and contrastive learning (as the more recent development of self-supervised learning). For the NHLBI DIR community, the teaching objectives are: * Introduce the basics of deep learning * Present in-math how DL model works * Provide practices to build your own model * Grow interest and improve community awareness * Prepare trainees and fellows for DL related jobs For the student point of view, you will gradually learn the concepts and algorithms behind the deep neural network and master the tools required to build and train models. For every lecture, it comes with a reading list to broaden the understanding. Assignments consist of questions and coding components and will be opened to student during teaching weeks. The focus is the understanding of basic concepts and how to design and apply deep learning to different problems. We will use Pytorch . So it is a good idea to get yourself familiar with this package . Prerequisites Please review the mathematics for deep learning and learn basic Python and Numpy. mathematics for deep learning Python, a more comprehensive book Python Crash Course, one of the easiest tutorial Numpy Pytorch tutorial Debug python program using VSCode Instructors Hui Xue, hui.xue@nih.gov David Hansen, davidchansen@gradientsoftware.net Schedule Starting on the week of Sep 13, 2021 Lecture, every Wed, 11:00am-12:30pm, US EST time https://us02web.zoom.us/j/2417250866?pwd=Zm9aTmgyZjNXdGRtUitmM0RGVXZPQT09 Q&A session, every Friday, 11:00am-12:00pm, US EST time https://us02web.zoom.us/j/2417250866?pwd=Zm9aTmgyZjNXdGRtUitmM0RGVXZPQT09 Course projects The course project is a great opportunity to apply deep learning to a problem of your interests. Given the very diverse background of course participants, it is of interests to see what deep learning may do in your domain. I would encourage you to think about a few ideas about how to apply DL to a problem in your domain. To get started, try to ask questions such as: Do I have a clear goal for what the model should do? E.g. segment a specific vessel and measure its diameter, or predict the outcome for a certain patient cohort, or improve imaging signal-noise-ratio or reduce radiation dose. Do I have data available, or whether I can start collecting data? Maybe data is generated in your workflow, but not saved. There are also openly available datasets. Is there a clear way to label the data? Maybe it is hard to come out a good data labelling strategy at the beginning. But can you get started and refine the data labelling after gaining more experience? How will you design the loss function and what is a possible model architecture? For these technical questions, there may be publications using deep learning for your domain. It is a good idea to review some papers to see how others formatted the problem. After building the model, will it be useful to improve your workflow or even become a deployed component? For example, a model to improve imaging SNR can be deployed and used regularly in experiments. If so, you will have a way to continuously adding new data to your dataset. You can work in a team of up to 3 people. You can consult any resources (e.g. github repo, books, papers, code by others etc.). But you are expected to collect, label/clean datasets, design and implement model, conduct the training, validating the model. In short, you are expected to drive a deep learning project . The course projects consist of three stages: proposal, milestone and final report. Proposal should be no more than one page. Try to state clearly about: what is the problem you are trying to solve with DL? What dataset will you use? If you are going to collect data, what is the strategy and timeline to curate a dataset? Ideas about model design and training. Finally, you should anticipate the expected outcome. Milestone should be no more than 3 pages, including: Introduction, Method, Dataset, Preliminary results and Discussion. You should clearly state the problem, technical route and possible further improvement. Final report should be up to 5 pages, including, Introduction, Related work if any, Method, Dataset, Training details, Results, Model deployment if applicable, Discussion, and Conclusion. Consider this final report to be the fist draft of your deep learning paper submitted to a good journal in your domain! Syllabus Prologue Why do we want to spend hours in learning deep learning (DL)? I can articulate one reason: Deep Learning is a set of key technique which can be applied to many fields, from mobile phone to medical imaging, from robotics to online shopping, from new drug discovery to genomics. What is really amazing to me is that in this wave of technological revolution, the same set of technique, deep neural network, is solving many challenging problems which are drastically different. Yes, it is the same set of algorithms, software toolboxes and knowledge base are applied, reporting state-of-the-art performance. This makes learning deep learning rewardable, because you will master something which can be widely applied and mostly likely will stay that way in the decades to come. According to ARK's research, deep learning will add $30 trillion to the global equity market capitalization during the next 15-20 years*. No something which should be ignored! However, there are difficulties along the way. Often, more than superficial level of understanding of DL is required, if you want to find a notch to apply DL in your field and if no one has done this before you. There will not be pre-trained models which you can download. One has to understand the problem and design the model, invent new loss functions and put all pieces together to build a DL solution. Your solution needs to prove its value in deployment and gets better over time. This course is to equip you with required knowledge to understand and apply DL by teaching how the deep neural network models work and by reviewing many DL architectures and applications. My hope is after this learning process, domain experts will feel confident to apply DL to their specific problems and datasets. Video Click here Slides PDF Download Suggested Reading *For big pictures and where DL can fit, Ark Big Idea Artificial Intelligence Index Report 2021, AI report Lecture 1 We start by motivating the deep learning for its broad applicability and future growth, and then introduce deep learning as a data driven approach. The basic terminology of neural network are reviewed. We set the stage for future discussion to introduce the binary and multi-class classification problems and the multi-layer perceptron (MLP) network. Other topics covered in this lecture include matrix broadcasting, universal approximation, logits, activation function etc. Video Click here Slides PDF Download Suggested Reading The same three authors wrote these two papers at the beginning of DL revolution and now. It is interesting to read and compare them. Deep learning, Nature volume 521, 436\u2013444 (2015) Deep learning for all Lecture 2 This lecture introduces the concept of loss function to evaluate how well our model fits the data. The process to adjust model parameters to fit the data is called optimization. Gradient descent is a common algorithm used to optimize the model parameters, given the input dataset. This process is the training. We will review different training algorithms and introduce the concepts of training and testing. To measure our model performance in training and testing datasets, the bias and variance of model should be estimated. Other concepts introduced in this lecture include regularization, under-fitting, over-fitting, batch and mini-batch etc. Video Click here Slides PDF Download Suggested Reading Notes on optimization Loss functions in deep learning by Artem Oppermann Bias and variance Lecture 3 The key step to train a model is to follow the negative gradient direction to reduce the loss. But how do we compute the gradient direction? Through a process called the back propagation or backprop in short. This lecture discusses the backprop in detail. Backprop is based on two ideas: chain rule of derivative and divide-and-conquer. It allows us to compute complex derivative from loss to every learnable parameters in the model. We will not review GPU devices for deep learning in lectures. Please review two documents in this week's reading list. Video To be added Slides PDF Download Suggested Reading How backprop works Derivatives of tensor Autograd in Pytorch GPU in Pytorch GPU for deep learning Assignment 1 Download the Assignment 1 In this assignment, you will be asked to implement the multi-layer perceptron model and cross-entropy loss. The coding problem will require the implementation for both forward pass and backprop. The gradient descent is used to train the model for higher classification accuracy. We will not use deep learning framework in this assignment, but will use Python+Numpy combination. The goal is to make sure the thorough understanding of mathematics and numeric technique necessary for a classic neural network. Also, it is to encourage one to get familiar with python coding. This assignment introduces the regression test for model training and evaluation. The Pytest is used for the regression test purpose. Download the linear regression demo Lecture 4 This lecture will finish our discussion on different optimization algorithms. We will introduce a few new methods and compare their pros and cons. The concept of hyper-parameter is explained, where a very important one is the learning rate. Different learning rate scheduling strategies are discussed in this lecture and help boost training performance. To search a good configuration of hyper-parameters, we will discuss coarse-to-fine, hyper-band and Bayesian methods. We close the lecture by discussing bag of tricks to set up the training process and cross-validation. Video To be added Slides PDF Download Suggested Reading Optimization in deep learning Learning rate scheduler in Pytorch One-cycle learning rate scheduler One-cycle learning rate scheduler, post Hyper-parameter searching Set up training, chapter 40, 41, 42 DL experiment management Lecture 5 This lecture continues our discussion on training setup, with focus on handling data mismatching between training and test sets. The meaning and strategy to conduct error analysis are introduced. After finishing the training section, we discuss the method for data pre-processing and how to initialize the model parameters. The final section of this lecture introduces the deep learning debugging and iteration for model training. Tools for debugging are demonstrated. Video To be added Slides PDF Download Suggested Reading Data mismatching Data pre-processing Data transformation in TorchVision Checklist to debug NN Lecture 6 This lecture starts the convolutional neural network (CNN) by introducing the convolution operation and its application on image. Different variants of convolution is discussed, including stride, transpose, dilated CONV, 1D and 3D CONV, padding, and pooling. Image interpolation layer is introduced with other methods to up/downsample images. The batch normalization is discussed with other feature normalization methods. Two CNN architectures are analyzed, LeNet-5 and AlexNet, in the history of ImageNet challenge. Video To be added Slides PDF Download Suggested Reading CONV and its variants CNN explanation Introduction for batch norm, layer norm, group norm etc. Overview of NN feature normalization ImageNet Winning CNN Architectures Lecture 7 With the basics of CONV and CNN introduced in last lecture, we continue to go through the history of ImageNet competition and reviewed winning architectures until 2017 and go beyond for very latest developments, including ResNet and its variants, group convolution, mobile net, efficient net. We can learn key ideas to design and refine the CNN architectures. The second part of this lecture discusses applications of CNN, including two-stage and one-stage object detection, landmark detection, U-net for segmentation, denoising CNN and super-resolution CNN. Network compression is not discussed in the lecture. But you are encouraged to read more on this topic. Video To be added Slides PDF Download Suggested Reading ResNet paper ResNet with batch norm Introduction for batch norm, layer norm, group norm etc. Mobile, shuffle, effnet One-stage object detector ResUnet Intro for network compression Lecture 8 We start the discussion of recurrent neural network (RNN) in this lecture. The vanilla RNN is introduced, with two variants: multi-level RNN and bidirectional RNN. To overcome the vanishing gradient, long-short term memory (LSTM) and gated recurrent unit (GRU) modules are introduced. This lecture also introduce word embedding and sequence pre-processing techniques. The final discussion is on temporal convolution network with dilated CONV to process sequence data. Video To be added Slides PDF Download Suggested Reading RNN chapter LSTM and GRU The Unreasonable Effectiveness of Recurrent Neural Networks Lecture 9 This lecture is an exciting one to introduce the attention mechanism and latest transformer based models which had demonstrated great success in natural language processing applications and showing promising the computer vision tasks. The discussion starts by introducing the self-attention and transformer module in details and extend to seq-to-seq model. The BERT and GTP-2/3 architectures are reviewed with details for training and inference. This lecture ends with reviewing application of transformer to computer vision tasks. Video To be added Slides PDF Download Suggested Reading Attention and transformer Attention is all your need Seq model and CTC loss GPT as a few shot learner Vision transformer GPT playground Lecture 10 So far we had discussed discriminative models which learn the decision boundary to make prediction. In this lecture, we start to discuss the generative model which learns the data distribution and allows sampling from that distribution. We start by discussing the differences and links between generative and discriminative models. Then the G/D design of generative adversarial model is introduced. The GAN loss is analyzed in detail and its optimal solution is derived. Examples are presented for the significant progress in GAN framework. We further extend the discussion to conditional GAN and CycleGAN to review its flexibility. The lecture is concluded by presenting an usecase utilizing the CycleGAN with segmentation consistency for medical imaging application. Video To be added Slides PDF Download Suggested Reading GAN paper GAN talk Conditional GAN Cycle GAN Lecture 11 This lecture discusses two related topics in deep learning: adversarial examples and deep NN visualization. Shortly after the take-off of deep NN in 2021, people found these models can be fooled by images looking normal to human eyes, while models will misclassified them with very high confidence. These examples are called adversarial examples. The cause for this phenomenon is discussed in the lecture with methods to generate adversarial examples and how to defend it. Understanding what models learned can help, which leads to techniques for model visualization. In the second part of lecture, different model visualization methods are reviewed, including occlusion, saliency maps, Grad-CAM and guided backprop. Video To be added Slides PDF Download Suggested Reading Adversarial examples Adversarial machine learning Visualization of deep NN Grad Cam Lecture 12 Deep learning works great if we have large amount of clean, diverse and labelled data. What if we only have a small amount of labelled data or a lot of data without labelling? This lecture introduces some techniques for these more realistic scenario, including transfer learning, meta learning and contrastive learning. The algorithms and utilities of these methods are discussed with real-world examples. The contrastive learning, as the latest development in self-supervised learning, is of particular interests to enable the usage of large unlabeled data. The lecture introduces SimCLR and MoCo algorithms and discusses the contrastive learning as the backbone for multi-modality usecases. Video To be added Slides PDF Download Suggested Reading Self-supervised learning Learn to learn, Lil'Log Self-Supervised Representation Learning, Lil'Log MAML SimCLR, a fun paper to read SimCLR v2 Closing remarks During this course, we learned many deep learning models and algorithms, and gained mindsets to work with DL. In this closing remark, we step back to take a bird-eye view of what we had learned. The big picture is DL is becoming an essential technology to create substantial value to the society for the decade to come. To achieve this potential, the machine learning system and effective development/deployment iteration are more important, leading to the rise of MLOps. As domain experts, adding DL to your expertise is a good way to get started. For many of us, it may be the only feasible way. So some more resources are provided for continuous learning. Suggested Reading Machine Learning System Design MLOps An insightful post for future of AI society Moore's Law for Everything","title":"NHLBI 2021"},{"location":"nhlbi2021/#nhlbi-2021-fall","text":"This offering is for Division of Intramural (DIR), NHLBI in Fall, 2021. The course consists of 12 lectures. We will start from basics of neural networks, introduce the loss function, optimization and how to setup and manage the training session. The next section is the convolutional neural network for imaging and vision tasks. We will learn the recurrent neural network (RNN) for the sequence data. More recently, attention mechanism and transformer models (BERT, GPT family etc.) are very popular. They are introduced after RNN. We will teach generative model and in details the GAN (generative adversarial network). The technique to visualize the neural network is introduced to help understand how and why the neural network works. The course will end with a focus on how to handle \"small dataset\" usecase, as in many practical applications, we may not be able to acquire large labelled dataset. Three techniques are introduced, transfer learning, meta learning and contrastive learning (as the more recent development of self-supervised learning). For the NHLBI DIR community, the teaching objectives are: * Introduce the basics of deep learning * Present in-math how DL model works * Provide practices to build your own model * Grow interest and improve community awareness * Prepare trainees and fellows for DL related jobs For the student point of view, you will gradually learn the concepts and algorithms behind the deep neural network and master the tools required to build and train models. For every lecture, it comes with a reading list to broaden the understanding. Assignments consist of questions and coding components and will be opened to student during teaching weeks. The focus is the understanding of basic concepts and how to design and apply deep learning to different problems. We will use Pytorch . So it is a good idea to get yourself familiar with this package .","title":"NHLBI, 2021 Fall"},{"location":"nhlbi2021/#prerequisites","text":"Please review the mathematics for deep learning and learn basic Python and Numpy. mathematics for deep learning Python, a more comprehensive book Python Crash Course, one of the easiest tutorial Numpy Pytorch tutorial Debug python program using VSCode","title":"Prerequisites"},{"location":"nhlbi2021/#instructors","text":"Hui Xue, hui.xue@nih.gov David Hansen, davidchansen@gradientsoftware.net","title":"Instructors"},{"location":"nhlbi2021/#schedule","text":"Starting on the week of Sep 13, 2021 Lecture, every Wed, 11:00am-12:30pm, US EST time https://us02web.zoom.us/j/2417250866?pwd=Zm9aTmgyZjNXdGRtUitmM0RGVXZPQT09 Q&A session, every Friday, 11:00am-12:00pm, US EST time https://us02web.zoom.us/j/2417250866?pwd=Zm9aTmgyZjNXdGRtUitmM0RGVXZPQT09","title":"Schedule"},{"location":"nhlbi2021/#course-projects","text":"The course project is a great opportunity to apply deep learning to a problem of your interests. Given the very diverse background of course participants, it is of interests to see what deep learning may do in your domain. I would encourage you to think about a few ideas about how to apply DL to a problem in your domain. To get started, try to ask questions such as: Do I have a clear goal for what the model should do? E.g. segment a specific vessel and measure its diameter, or predict the outcome for a certain patient cohort, or improve imaging signal-noise-ratio or reduce radiation dose. Do I have data available, or whether I can start collecting data? Maybe data is generated in your workflow, but not saved. There are also openly available datasets. Is there a clear way to label the data? Maybe it is hard to come out a good data labelling strategy at the beginning. But can you get started and refine the data labelling after gaining more experience? How will you design the loss function and what is a possible model architecture? For these technical questions, there may be publications using deep learning for your domain. It is a good idea to review some papers to see how others formatted the problem. After building the model, will it be useful to improve your workflow or even become a deployed component? For example, a model to improve imaging SNR can be deployed and used regularly in experiments. If so, you will have a way to continuously adding new data to your dataset. You can work in a team of up to 3 people. You can consult any resources (e.g. github repo, books, papers, code by others etc.). But you are expected to collect, label/clean datasets, design and implement model, conduct the training, validating the model. In short, you are expected to drive a deep learning project . The course projects consist of three stages: proposal, milestone and final report. Proposal should be no more than one page. Try to state clearly about: what is the problem you are trying to solve with DL? What dataset will you use? If you are going to collect data, what is the strategy and timeline to curate a dataset? Ideas about model design and training. Finally, you should anticipate the expected outcome. Milestone should be no more than 3 pages, including: Introduction, Method, Dataset, Preliminary results and Discussion. You should clearly state the problem, technical route and possible further improvement. Final report should be up to 5 pages, including, Introduction, Related work if any, Method, Dataset, Training details, Results, Model deployment if applicable, Discussion, and Conclusion. Consider this final report to be the fist draft of your deep learning paper submitted to a good journal in your domain!","title":"Course projects"},{"location":"nhlbi2021/#syllabus","text":"","title":"Syllabus"},{"location":"nhlbi2021/#prologue","text":"Why do we want to spend hours in learning deep learning (DL)? I can articulate one reason: Deep Learning is a set of key technique which can be applied to many fields, from mobile phone to medical imaging, from robotics to online shopping, from new drug discovery to genomics. What is really amazing to me is that in this wave of technological revolution, the same set of technique, deep neural network, is solving many challenging problems which are drastically different. Yes, it is the same set of algorithms, software toolboxes and knowledge base are applied, reporting state-of-the-art performance. This makes learning deep learning rewardable, because you will master something which can be widely applied and mostly likely will stay that way in the decades to come. According to ARK's research, deep learning will add $30 trillion to the global equity market capitalization during the next 15-20 years*. No something which should be ignored! However, there are difficulties along the way. Often, more than superficial level of understanding of DL is required, if you want to find a notch to apply DL in your field and if no one has done this before you. There will not be pre-trained models which you can download. One has to understand the problem and design the model, invent new loss functions and put all pieces together to build a DL solution. Your solution needs to prove its value in deployment and gets better over time. This course is to equip you with required knowledge to understand and apply DL by teaching how the deep neural network models work and by reviewing many DL architectures and applications. My hope is after this learning process, domain experts will feel confident to apply DL to their specific problems and datasets.","title":"Prologue"},{"location":"nhlbi2021/#video","text":"Click here","title":"Video"},{"location":"nhlbi2021/#slides","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading","text":"*For big pictures and where DL can fit, Ark Big Idea Artificial Intelligence Index Report 2021, AI report","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-1","text":"We start by motivating the deep learning for its broad applicability and future growth, and then introduce deep learning as a data driven approach. The basic terminology of neural network are reviewed. We set the stage for future discussion to introduce the binary and multi-class classification problems and the multi-layer perceptron (MLP) network. Other topics covered in this lecture include matrix broadcasting, universal approximation, logits, activation function etc.","title":"Lecture 1"},{"location":"nhlbi2021/#video_1","text":"Click here","title":"Video"},{"location":"nhlbi2021/#slides_1","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_1","text":"The same three authors wrote these two papers at the beginning of DL revolution and now. It is interesting to read and compare them. Deep learning, Nature volume 521, 436\u2013444 (2015) Deep learning for all","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-2","text":"This lecture introduces the concept of loss function to evaluate how well our model fits the data. The process to adjust model parameters to fit the data is called optimization. Gradient descent is a common algorithm used to optimize the model parameters, given the input dataset. This process is the training. We will review different training algorithms and introduce the concepts of training and testing. To measure our model performance in training and testing datasets, the bias and variance of model should be estimated. Other concepts introduced in this lecture include regularization, under-fitting, over-fitting, batch and mini-batch etc.","title":"Lecture 2"},{"location":"nhlbi2021/#video_2","text":"Click here","title":"Video"},{"location":"nhlbi2021/#slides_2","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_2","text":"Notes on optimization Loss functions in deep learning by Artem Oppermann Bias and variance","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-3","text":"The key step to train a model is to follow the negative gradient direction to reduce the loss. But how do we compute the gradient direction? Through a process called the back propagation or backprop in short. This lecture discusses the backprop in detail. Backprop is based on two ideas: chain rule of derivative and divide-and-conquer. It allows us to compute complex derivative from loss to every learnable parameters in the model. We will not review GPU devices for deep learning in lectures. Please review two documents in this week's reading list.","title":"Lecture 3"},{"location":"nhlbi2021/#video_3","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_3","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_3","text":"How backprop works Derivatives of tensor Autograd in Pytorch GPU in Pytorch GPU for deep learning","title":"Suggested Reading"},{"location":"nhlbi2021/#assignment-1","text":"Download the Assignment 1 In this assignment, you will be asked to implement the multi-layer perceptron model and cross-entropy loss. The coding problem will require the implementation for both forward pass and backprop. The gradient descent is used to train the model for higher classification accuracy. We will not use deep learning framework in this assignment, but will use Python+Numpy combination. The goal is to make sure the thorough understanding of mathematics and numeric technique necessary for a classic neural network. Also, it is to encourage one to get familiar with python coding. This assignment introduces the regression test for model training and evaluation. The Pytest is used for the regression test purpose. Download the linear regression demo","title":"Assignment 1"},{"location":"nhlbi2021/#lecture-4","text":"This lecture will finish our discussion on different optimization algorithms. We will introduce a few new methods and compare their pros and cons. The concept of hyper-parameter is explained, where a very important one is the learning rate. Different learning rate scheduling strategies are discussed in this lecture and help boost training performance. To search a good configuration of hyper-parameters, we will discuss coarse-to-fine, hyper-band and Bayesian methods. We close the lecture by discussing bag of tricks to set up the training process and cross-validation.","title":"Lecture 4"},{"location":"nhlbi2021/#video_4","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_4","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_4","text":"Optimization in deep learning Learning rate scheduler in Pytorch One-cycle learning rate scheduler One-cycle learning rate scheduler, post Hyper-parameter searching Set up training, chapter 40, 41, 42 DL experiment management","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-5","text":"This lecture continues our discussion on training setup, with focus on handling data mismatching between training and test sets. The meaning and strategy to conduct error analysis are introduced. After finishing the training section, we discuss the method for data pre-processing and how to initialize the model parameters. The final section of this lecture introduces the deep learning debugging and iteration for model training. Tools for debugging are demonstrated.","title":"Lecture 5"},{"location":"nhlbi2021/#video_5","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_5","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_5","text":"Data mismatching Data pre-processing Data transformation in TorchVision Checklist to debug NN","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-6","text":"This lecture starts the convolutional neural network (CNN) by introducing the convolution operation and its application on image. Different variants of convolution is discussed, including stride, transpose, dilated CONV, 1D and 3D CONV, padding, and pooling. Image interpolation layer is introduced with other methods to up/downsample images. The batch normalization is discussed with other feature normalization methods. Two CNN architectures are analyzed, LeNet-5 and AlexNet, in the history of ImageNet challenge.","title":"Lecture 6"},{"location":"nhlbi2021/#video_6","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_6","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_6","text":"CONV and its variants CNN explanation Introduction for batch norm, layer norm, group norm etc. Overview of NN feature normalization ImageNet Winning CNN Architectures","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-7","text":"With the basics of CONV and CNN introduced in last lecture, we continue to go through the history of ImageNet competition and reviewed winning architectures until 2017 and go beyond for very latest developments, including ResNet and its variants, group convolution, mobile net, efficient net. We can learn key ideas to design and refine the CNN architectures. The second part of this lecture discusses applications of CNN, including two-stage and one-stage object detection, landmark detection, U-net for segmentation, denoising CNN and super-resolution CNN. Network compression is not discussed in the lecture. But you are encouraged to read more on this topic.","title":"Lecture 7"},{"location":"nhlbi2021/#video_7","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_7","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_7","text":"ResNet paper ResNet with batch norm Introduction for batch norm, layer norm, group norm etc. Mobile, shuffle, effnet One-stage object detector ResUnet Intro for network compression","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-8","text":"We start the discussion of recurrent neural network (RNN) in this lecture. The vanilla RNN is introduced, with two variants: multi-level RNN and bidirectional RNN. To overcome the vanishing gradient, long-short term memory (LSTM) and gated recurrent unit (GRU) modules are introduced. This lecture also introduce word embedding and sequence pre-processing techniques. The final discussion is on temporal convolution network with dilated CONV to process sequence data.","title":"Lecture 8"},{"location":"nhlbi2021/#video_8","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_8","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_8","text":"RNN chapter LSTM and GRU The Unreasonable Effectiveness of Recurrent Neural Networks","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-9","text":"This lecture is an exciting one to introduce the attention mechanism and latest transformer based models which had demonstrated great success in natural language processing applications and showing promising the computer vision tasks. The discussion starts by introducing the self-attention and transformer module in details and extend to seq-to-seq model. The BERT and GTP-2/3 architectures are reviewed with details for training and inference. This lecture ends with reviewing application of transformer to computer vision tasks.","title":"Lecture 9"},{"location":"nhlbi2021/#video_9","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_9","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_9","text":"Attention and transformer Attention is all your need Seq model and CTC loss GPT as a few shot learner Vision transformer GPT playground","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-10","text":"So far we had discussed discriminative models which learn the decision boundary to make prediction. In this lecture, we start to discuss the generative model which learns the data distribution and allows sampling from that distribution. We start by discussing the differences and links between generative and discriminative models. Then the G/D design of generative adversarial model is introduced. The GAN loss is analyzed in detail and its optimal solution is derived. Examples are presented for the significant progress in GAN framework. We further extend the discussion to conditional GAN and CycleGAN to review its flexibility. The lecture is concluded by presenting an usecase utilizing the CycleGAN with segmentation consistency for medical imaging application.","title":"Lecture 10"},{"location":"nhlbi2021/#video_10","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_10","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_10","text":"GAN paper GAN talk Conditional GAN Cycle GAN","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-11","text":"This lecture discusses two related topics in deep learning: adversarial examples and deep NN visualization. Shortly after the take-off of deep NN in 2021, people found these models can be fooled by images looking normal to human eyes, while models will misclassified them with very high confidence. These examples are called adversarial examples. The cause for this phenomenon is discussed in the lecture with methods to generate adversarial examples and how to defend it. Understanding what models learned can help, which leads to techniques for model visualization. In the second part of lecture, different model visualization methods are reviewed, including occlusion, saliency maps, Grad-CAM and guided backprop.","title":"Lecture 11"},{"location":"nhlbi2021/#video_11","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_11","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_11","text":"Adversarial examples Adversarial machine learning Visualization of deep NN Grad Cam","title":"Suggested Reading"},{"location":"nhlbi2021/#lecture-12","text":"Deep learning works great if we have large amount of clean, diverse and labelled data. What if we only have a small amount of labelled data or a lot of data without labelling? This lecture introduces some techniques for these more realistic scenario, including transfer learning, meta learning and contrastive learning. The algorithms and utilities of these methods are discussed with real-world examples. The contrastive learning, as the latest development in self-supervised learning, is of particular interests to enable the usage of large unlabeled data. The lecture introduces SimCLR and MoCo algorithms and discusses the contrastive learning as the backbone for multi-modality usecases.","title":"Lecture 12"},{"location":"nhlbi2021/#video_12","text":"To be added","title":"Video"},{"location":"nhlbi2021/#slides_12","text":"PDF Download","title":"Slides"},{"location":"nhlbi2021/#suggested-reading_12","text":"Self-supervised learning Learn to learn, Lil'Log Self-Supervised Representation Learning, Lil'Log MAML SimCLR, a fun paper to read SimCLR v2","title":"Suggested Reading"},{"location":"nhlbi2021/#closing-remarks","text":"During this course, we learned many deep learning models and algorithms, and gained mindsets to work with DL. In this closing remark, we step back to take a bird-eye view of what we had learned. The big picture is DL is becoming an essential technology to create substantial value to the society for the decade to come. To achieve this potential, the machine learning system and effective development/deployment iteration are more important, leading to the rise of MLOps. As domain experts, adding DL to your expertise is a good way to get started. For many of us, it may be the only feasible way. So some more resources are provided for continuous learning.","title":"Closing remarks"},{"location":"nhlbi2021/#suggested-reading_13","text":"Machine Learning System Design MLOps An insightful post for future of AI society Moore's Law for Everything","title":"Suggested Reading"},{"location":"setup_ubuntu/","text":"Setup for deep learning development, Ubuntu 20.04 This guideline provides tutorial for how to set up deep learning development environment for Ubuntu 20.04 . To complete the assignments of this course, it is required for all participants to set up their deep learning environment. Since the later problems involve more complicated model and larger datasets, the Nvidia GPU is required. This guide demonstrates the setup for GPU as well as the python components. As an overview, following software and tools are used in this course: * Python3.8 * Pytorch 1.9, for cuda 11 * VSCode for coding and debugging * wandb (weights and biases) for the experiment management and hyperparameter searching * To remote log into the graphic desktop of Ubuntu system, X2go server and client are used Install NVIDIA driver First, install the nvidia driver for the GPU cards. If your GPU is already set up, you can skip this step. # remote old installation if any sudo apt-get --purge remove cuda* sudo apt-get remove --purge nvidia-* # add nvidia driver ppa sudo add-apt-repository ppa:graphics-drivers/ppa # update software cache sudo apt update sudo apt upgrade -y # install driver sudo apt-get install ubuntu-drivers-common # recommend to install driver version 470 sudo ubuntu-drivers install 470 # then reboot the computer sudo reboot now Install dependent software To help with the setup, a bash file is prepared to install all dependencies, including software packages and python packages: install_ubuntu_dependencies.sh Please copy this file to your computer and run: # suppose the working directory is ~/software cd ~/software wget https://deeplearningcrashcourse.org/setup/install_ubuntu_dependencies.sh dos2unix ./install_ubuntu_dependencies.sh # in case the line ending format causes problem sudo sh ./install_ubuntu_dependencies.sh Install cuda # suppose the working folder is ~/software mkdir ~/software cd ~/software wget https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run sudo sh ./cuda_11.4.0_470.42.01_linux.run When installing CUDA, first select \"Continue\": then type in \"accept\": make sure uncheck driver: then move the cursor to \"Install\": Third, check the GPU card is working # run this command nvidia-smi # you may want to enable the persistent mode to save time sudo nvidia-smi -pm 1 If the GPU is up and running, you should see information like this: Install pytorch sudo pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html After the installation, please check pytorch is installed properly: python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\" If pytorch is installed correctly, you should see this: Set up the remote desktop This part of setup is to enable your windows computer to remote login the ubuntu computer with graphic interface. First, on the ubuntu side, install x2go server and mate desktop # install x2go server sudo add-apt-repository ppa:x2go/stable sudo apt-get update sudo apt-get install x2goserver x2goserver-xsession # install ubuntu mate desktop sudo apt install tasksel sudo tasksel install ubuntu-mate-desktop # reboot sudo reboot now Second, on the Windows side (suppose you are using MS Windows to log into the linux computer), download and install the x2go client . Last, start the x2go client to remote into the ubuntu server: set up the x2go connection as this: make sure put in the ip in this \"Host\" and select MATE in the \"Session type\". After clicking OK , the session should be saved: Click this session and type in the password: You now log into the ubuntu remote serve with mate desktop:","title":"Ubuntu"},{"location":"setup_ubuntu/#setup-for-deep-learning-development-ubuntu-2004","text":"This guideline provides tutorial for how to set up deep learning development environment for Ubuntu 20.04 . To complete the assignments of this course, it is required for all participants to set up their deep learning environment. Since the later problems involve more complicated model and larger datasets, the Nvidia GPU is required. This guide demonstrates the setup for GPU as well as the python components. As an overview, following software and tools are used in this course: * Python3.8 * Pytorch 1.9, for cuda 11 * VSCode for coding and debugging * wandb (weights and biases) for the experiment management and hyperparameter searching * To remote log into the graphic desktop of Ubuntu system, X2go server and client are used","title":"Setup for deep learning development, Ubuntu 20.04"},{"location":"setup_ubuntu/#install-nvidia-driver","text":"First, install the nvidia driver for the GPU cards. If your GPU is already set up, you can skip this step. # remote old installation if any sudo apt-get --purge remove cuda* sudo apt-get remove --purge nvidia-* # add nvidia driver ppa sudo add-apt-repository ppa:graphics-drivers/ppa # update software cache sudo apt update sudo apt upgrade -y # install driver sudo apt-get install ubuntu-drivers-common # recommend to install driver version 470 sudo ubuntu-drivers install 470 # then reboot the computer sudo reboot now","title":"Install NVIDIA driver"},{"location":"setup_ubuntu/#install-dependent-software","text":"To help with the setup, a bash file is prepared to install all dependencies, including software packages and python packages: install_ubuntu_dependencies.sh Please copy this file to your computer and run: # suppose the working directory is ~/software cd ~/software wget https://deeplearningcrashcourse.org/setup/install_ubuntu_dependencies.sh dos2unix ./install_ubuntu_dependencies.sh # in case the line ending format causes problem sudo sh ./install_ubuntu_dependencies.sh","title":"Install dependent software"},{"location":"setup_ubuntu/#install-cuda","text":"# suppose the working folder is ~/software mkdir ~/software cd ~/software wget https://developer.download.nvidia.com/compute/cuda/11.4.0/local_installers/cuda_11.4.0_470.42.01_linux.run sudo sh ./cuda_11.4.0_470.42.01_linux.run When installing CUDA, first select \"Continue\": then type in \"accept\": make sure uncheck driver: then move the cursor to \"Install\": Third, check the GPU card is working # run this command nvidia-smi # you may want to enable the persistent mode to save time sudo nvidia-smi -pm 1 If the GPU is up and running, you should see information like this:","title":"Install cuda"},{"location":"setup_ubuntu/#install-pytorch","text":"sudo pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html After the installation, please check pytorch is installed properly: python3 -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\" If pytorch is installed correctly, you should see this:","title":"Install pytorch"},{"location":"setup_ubuntu/#set-up-the-remote-desktop","text":"This part of setup is to enable your windows computer to remote login the ubuntu computer with graphic interface. First, on the ubuntu side, install x2go server and mate desktop # install x2go server sudo add-apt-repository ppa:x2go/stable sudo apt-get update sudo apt-get install x2goserver x2goserver-xsession # install ubuntu mate desktop sudo apt install tasksel sudo tasksel install ubuntu-mate-desktop # reboot sudo reboot now Second, on the Windows side (suppose you are using MS Windows to log into the linux computer), download and install the x2go client . Last, start the x2go client to remote into the ubuntu server: set up the x2go connection as this: make sure put in the ip in this \"Host\" and select MATE in the \"Session type\". After clicking OK , the session should be saved: Click this session and type in the password: You now log into the ubuntu remote serve with mate desktop:","title":"Set up the remote desktop"},{"location":"setup_win10/","text":"Setup for deep learning development, Windows 10 This guideline provides tutorial for how to set up deep learning development environment for Windows 10. To complete the assignments of this course, it is required for all participants to set up their deep learning environment. Since the later problems involve more complicated model and larger datasets, the Nvidia GPU is required. This guide demonstrates the setup for GPU as well as the python components. As an overview, following software and tools are used in this course: * Python3.8 * Pytorch 1.9, for cuda 11 * VSCode for coding and debugging * wandb (weights and biases) for the experiment management and hyperparameter searching * To remote log into the graphic desktop of Ubuntu system, X2go server and client are used Install NVIDIA driver and CUDA Download the CUDA installer for windows Double click cuda_11.4.0_471.11_win10.exe Follow on-screen prompts Install python and pip Download the python 3.8 installer Unzip and double click Use all default values and install this package Install python pip cd c:\\temp curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py If you get this error: it can be fixed by: Go to windows run. Type %appdata% Go to the folder pip and edit the pip.ini file. If the folder doesn't exist create one and also create a pip.ini file and edit in a text editor. Add the following : [global] trusted-host = pypi.python.org pypi.org files.pythonhosted.org raw.githubusercontent.com github.com download.pytorch.org Install vscode Go to VSCode page and download and install this software for windows 64 bit. Install python packages To help with the setup, a bash file is prepared to install all dependencies, including software packages and python packages: install_windows_dependencies.bat Please copy this file to your computer and open a cmd window : %% suppose the working directory is c:\\temp cd c:\\temp install_windows_dependencies.bat After the installation, please check pytorch is installed properly: python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\" If pytorch is installed correctly, you should see this:","title":"Windows 10"},{"location":"setup_win10/#setup-for-deep-learning-development-windows-10","text":"This guideline provides tutorial for how to set up deep learning development environment for Windows 10. To complete the assignments of this course, it is required for all participants to set up their deep learning environment. Since the later problems involve more complicated model and larger datasets, the Nvidia GPU is required. This guide demonstrates the setup for GPU as well as the python components. As an overview, following software and tools are used in this course: * Python3.8 * Pytorch 1.9, for cuda 11 * VSCode for coding and debugging * wandb (weights and biases) for the experiment management and hyperparameter searching * To remote log into the graphic desktop of Ubuntu system, X2go server and client are used","title":"Setup for deep learning development, Windows 10"},{"location":"setup_win10/#install-nvidia-driver-and-cuda","text":"Download the CUDA installer for windows Double click cuda_11.4.0_471.11_win10.exe Follow on-screen prompts","title":"Install NVIDIA driver and CUDA"},{"location":"setup_win10/#install-python-and-pip","text":"Download the python 3.8 installer Unzip and double click Use all default values and install this package","title":"Install python and pip"},{"location":"setup_win10/#install-python-pip","text":"cd c:\\temp curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py If you get this error: it can be fixed by: Go to windows run. Type %appdata% Go to the folder pip and edit the pip.ini file. If the folder doesn't exist create one and also create a pip.ini file and edit in a text editor. Add the following : [global] trusted-host = pypi.python.org pypi.org files.pythonhosted.org raw.githubusercontent.com github.com download.pytorch.org","title":"Install python pip"},{"location":"setup_win10/#install-vscode","text":"Go to VSCode page and download and install this software for windows 64 bit.","title":"Install vscode"},{"location":"setup_win10/#install-python-packages","text":"To help with the setup, a bash file is prepared to install all dependencies, including software packages and python packages: install_windows_dependencies.bat Please copy this file to your computer and open a cmd window : %% suppose the working directory is c:\\temp cd c:\\temp install_windows_dependencies.bat After the installation, please check pytorch is installed properly: python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\" If pytorch is installed correctly, you should see this:","title":"Install python packages"}]}